{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scroll': True, 'start_slideshow_at': 'selected', 'theme': 'night'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = '/Users/jmk/anaconda2/envs/data601/etc/jupyter/nbconfig'\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'night',\n",
    "              'scroll': True,\n",
    "              #'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Selection\n",
    "Often we have more features than we know what to do with.  Generally speaking the more features you use in your model, the more data you need to help sort through the various permutations.  This is referred to as the \"curse of dimensionality\" (Bellman) and is described as:\n",
    "\n",
    "> In machine learning problems that involve ... data samples in a **high-dimensional feature space** with each feature having a number of possible values, **an enormous amount of training data is required to ensure that there are several samples with each combination of values**. With a fixed number of training samples, the predictive power reduces as the dimensionality increases, and this is known as Hughes phenomenon (named after Gordon F. Hughes).\n",
    "\n",
    "... both from https://en.m.wikipedia.org/wiki/Curse_of_dimensionality\n",
    "\n",
    "Finding a way to quickly identify features that don't offer predictive power lets us _iterate faster_ with _less data_ which lets us experiment more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some approaches to feature selection\n",
    "\n",
    "* Removing features with _low variance_ (`VarianceThreshold()`)\n",
    "* Univariate feature selection (`SelectKBest`)\n",
    "* Recursive feature elimination (`RFE`)\n",
    "* Using a simple, sparser model to identify which features have predictive power (`SelectFromModel`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VarianceThreshold\n",
    "\n",
    "This approach simply analyzes the variance of each feature and removes any features that have a low variance.  Example:  Let's say we're analyzing which bikes get rented at a bicycle shop.  The number of seats on the bicycle is likely always `1`, so it doesn't help us decide anything.\n",
    "\n",
    "The intuition is that they don't change much, so they can't have predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], \n",
    "     [0, 1, 0], \n",
    "     [1, 0, 0], \n",
    "     [0, 1, 1], \n",
    "     [0, 1, 0], \n",
    "     [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest\n",
    "Removes all but the $k$ highest scoring features.  \"Highest scoring\" is  configurable using any function of `(X, y)` that returns a set of scores per feature.\n",
    "\n",
    "Predefined scoring functions include `chi2` ($\\chi^2$ test, testing for the likelihood that the relationship between each variable and the outcome is due to that feature), `f_classif` (ANOVA or analysis of variation), and mutual information metrics for both classification and regression problems.  User-defined scoring functions can be passed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 10.81782088,   3.59449902, 116.16984746,  67.24482759]),\n",
       " array([4.47651499e-03, 1.65754167e-01, 5.94344354e-26, 2.50017968e-15]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "chi2(X, y)  \n",
    "#  Returns two arrays:  chi2 scores and p-values, each per _feature_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the final dimensionality of each sample is now 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE\n",
    "Recursive feature elimination (RFE) uses an external estimator that assigns weights to features (e.g. the coefficients of a linear model), RFE selects features by recursively considering smaller and smaller sets of features.  \n",
    "\n",
    "It does this by training the external estimator and then dropping the least important features (either through `.coef_` or `.feature_importances_`).  \n",
    "\n",
    "It then repeats this procedure until the desired number of features is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64 50 31 23 10 17 34 51 57 37 30 43 14 32 44 52 54 41 19 15 28  8 39 53\n",
      " 55 45  9 18 20 38  1 59 63 42 25 35 29 16  2 62 61 40  5 11 13  6  4 58\n",
      " 56 47 26 36 24  3 22 48 60 49  7 27 33 21 12 46]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.images.reshape((len(digits.images), -1))\n",
    "y = digits.target\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "rfe.fit(X, y)\n",
    "ranking = rfe.ranking_.reshape(digits.images[0].shape)\n",
    "\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectFromModel\n",
    "\n",
    "`SelectFromModel` is similar to `RFE`, but instead of deciding on how many features to retain and removing features until we reach that limit, `SelectFromModel` uses a threshold on the `coef_` or `feature_importances_` values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(X.shape) \n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of these cases, we're selecting features _from the original set_ that we're going to keep.  We didn't compute any new feature, we simply _select_ from the available set.\n",
    "\n",
    "An alternative approach called _dimensionality reduction_ lets us synthesize new features from the existing features.  We'll discuss it next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
