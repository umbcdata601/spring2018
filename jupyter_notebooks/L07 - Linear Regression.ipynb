{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scroll': True, 'start_slideshow_at': 'selected', 'theme': 'night'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = '/Users/jmk/anaconda2/envs/data601/etc/jupyter/nbconfig'\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'night',\n",
    "              'scroll': True,\n",
    "              #'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable, $X$. \n",
    "\n",
    "Mathematically we can write this as: $Y = B_0 + B_1X $\n",
    "\n",
    "That is, $Y$ is \"approximately modeled as\" a linear function of $X$.\n",
    "\n",
    "$B_0$ and $B_1$ are terms that represent the intercept and slope of the line, respectively.  (Recall $y = mx + b$)  Note:  $B_0$ is referred to as the _bias_ term.\n",
    "\n",
    "These are call the _coefficients_ or _parameters_ of the model.  They are the _unknowns_ we want to estimate with the training process.\n",
    "\n",
    "Once we know these values, we can _predict_ $y$ for unknown values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  How does Linear Regression Work?\n",
    "\n",
    "We want to obtain values of the coefficients so that the linear model \"fits the data well\".  We intuitively define this as the line that follows the \"shape\" of the training data.  \n",
    "\n",
    "More formally, it does the best job of predicting the values of all of the training samples as closely as possible.\n",
    "\n",
    "To do this, we need a way to:\n",
    "\n",
    "* Define \"closely\" so that we can measure it\n",
    "* Define a _search procedure_ that will let us explore the parameter space to find the \"best\" solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Defining Closeness\n",
    "\n",
    "The most common way to define \"close\" is measuring something called the _least squares_ criterion.  Let $\\hat{y_i} = \\hat{B_0} + \\hat{B_1}x_i$ be a prediction of the output, $y$ for a given input $x_i$.  \n",
    "\n",
    "We'll define the _residual_ as the difference between what the current model gives us and the \"right\" answer, $y_i$.  That is, $e_i = y_i - \\hat{y_i}$  This is the distance, in $y$, of the line from the right answer, as shown below.\n",
    "\n",
    "![linear-regression-residuals.png](linear-regression-residuals.png)\n",
    "\n",
    "We define the _residual sum of squares (RSS)_ as the _sum of the squares_ of those residuals.  That is: $\\sum{(y_i - \\hat{y_i})^2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The \"closest\" match is the one that provides the minimum value of _RSS_.\n",
    "\n",
    "![linear-regression-optimization.png](linear-regression-optimization.png)\n",
    "\n",
    "This is what we call an _optimization problem_.  The goal is to find the values of the parameters that minimizes the RSS \"cost\" of the solution.  That is, of all the solutions, we want to find the best.  But we have to go looking for it in the search space above.\n",
    "\n",
    "Note that every point on that mesh surface represents a pair of parameters.\n",
    "\n",
    "That is, each point there represents a possible hypothesis.  And they're all laid out by the cost function (RSS) that we've defined.\n",
    "\n",
    "Because this is a squared value, it's a _continuous function_ (read:  smoothly curved surface) which means we can use derivatives to calculate the _gradient_ and move in the steepest direction towards the bottom of the hill.  Because the space is _convex_, in this case, we're guaranteed a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that this doesn't mean it's the _best_ solution, it's just the best that this _model_ can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
