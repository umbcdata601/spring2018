{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scroll': True, 'start_slideshow_at': 'selected', 'theme': 'night'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = '/Users/jmk/anaconda2/envs/data601/etc/jupyter/nbconfig'\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'night',\n",
    "              'scroll': True,\n",
    "              #'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So Machine Learning is really three separate steps\n",
    "\n",
    "* Get labelled training data\n",
    "* Convert your training data into n-dimensional vectors\n",
    "* Run the ML algorithm\n",
    "\n",
    "The second step is what we call \"feature engineering\".  It's the primary way that we encode human knowledge (wisdom?) into the ML process.  We pick which things that algorithm can look at but we also need to be able to describe them in a way that's meaningful to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get labelled training data\n",
    "\n",
    "Supervised machine learning uses the \"answer key\" to help it adjust the way it makes decisions.\n",
    "\n",
    "In order to use supervised machine learning methods, we _must_ get labelled training data.\n",
    "\n",
    "This normally means either a known value over time or a label to place it into a class (e.g. spam vs. not-spam)\n",
    "\n",
    "How can we get labelled training data:\n",
    "* Find a dataset that includes labels\n",
    "* Label it by hand ourselves\n",
    "* Trick users into labelling it for us\n",
    "* Hire users to label it for you (e.g. Amazon Mechanical Turk, Crowdflower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find a dataset that includes labels\n",
    "\n",
    "* Talk to the customer\n",
    "* Use google\n",
    "* Check places like kaggle, the UCI Machine Learning Library, etc.\n",
    "* Ask at the opendata stack exchange\n",
    "* Explore other open sources of data (data.gov, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Label it by hand ourselves\n",
    "\n",
    "* Assumes you have enough domain expertise\n",
    "  * Probably fine if it's identifying dogs vs. cats\n",
    "  * Not fine if it's diagnosing radiological images\n",
    "* Slow \n",
    "* Time consuming\n",
    "* Expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trick users into labelling it for us\n",
    "\n",
    "Can we find a part of the average users workflow that gives us the answer already?\n",
    "\n",
    "* Conversion testing for web sites.  \n",
    "  * We can track mouse movements and clicks to know if they did or didn't \"convert\".\n",
    "  * Some cool tools out there for this (heapanalytics is one)\n",
    "* House number identification\n",
    "  * reCaptcha works data labelling into the captcha process\n",
    "\n",
    "* Takes time to collect (unless you already have data logged)\n",
    "* May take effort to instrument the system to record the desired behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hire Annotators\n",
    "* Can be expensive, depending on expertise required\n",
    "* Requires carefully defining the task at hand\n",
    "* Still have to pay for mistakes\n",
    "* Takes time to coordinate\n",
    "* Crowdsourcing is an option (e.g. mechanical turk, crowdflower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Convert Data Into n-dimensional Vectors\n",
    "\n",
    "* In order to send data as input to ML algorithms, we must conver it to a vector of numbers\n",
    "* For _quantitative_ variables, this is normally straightforward\n",
    "* What about categorical variables?  What could we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ordinal variables, while categorical, are probably ok.  \n",
    "  * e.g. \"Strongly agree\", \"Agree\", \"Neither agree nor disagree\", \"Disagree\", \"Strongly disagree\"\n",
    "  * Because they're _ordinal_, we can assign them to a sequence of integers\n",
    "* What about _nominal_ variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Categorical variables:  One-hot encoding\n",
    "\n",
    "Let's say we have bicycles to rent and they come in three colors: red, green, and blue.\n",
    "\n",
    "How can we turn that into a value in a numerical vector?  If we just assign random numbers to them, the algorithm can get confused since it will assume that those distances are meaningful.\n",
    "\n",
    "What we really want is for \"redness\" to be a dimension, \"blueness\" to be a dimension, and \"greenness\" to be a dimension.\n",
    "\n",
    "To do this we use _one-hot encoding_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "\n",
    "#  This learns that red will be [0,0,1], blue will be [1, 0, 0], etc.\n",
    "binarizer.fit(['red', 'blue', 'green'])\n",
    "\n",
    "#  This takes what it learned previously and replaces each string with it's replacement\n",
    "#  in one-hot encoding\n",
    "binarizer.transform(['red', 'blue', 'green', 'red', 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue', 'green', 'red'], dtype='<U5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we need to merge these values back into our array of features.\n",
    "\n",
    "Note that the positions of the various features need to be the same for each data sample.  That is, color can't be the first three elements of one sample and the last three of the next.\n",
    "\n",
    "Fortunately for us, this is so common that sklearn has a built-in mechanism for taking an existing feature matrix and one-hot encoding some fields but not necessarily all:  The `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City samples:  [0 2 1 1 1 3]\n",
      "Country samples:  [2 2 2 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "enc = OneHotEncoder()\n",
    "city_enc = LabelEncoder()\n",
    "country_enc = LabelEncoder()\n",
    "\n",
    "city_enc.fit(['Atlanta', 'Baltimore',  'Zurich', 'Charlotte'])\n",
    "country_enc.fit(['USA', 'Switzerland', 'Germany'])\n",
    "\n",
    "samples = [['Atlanta', 'USA'],\n",
    "           ['Charlotte', 'USA'],\n",
    "           ['Baltimore', 'USA'],\n",
    "           ['Baltimore', 'Germany'],\n",
    "           ['Baltimore', 'Switzerland'],\n",
    "           ['Zurich', 'Switzerland']]\n",
    "city_samples = city_enc.transform([sample[0] for sample in samples])\n",
    "print('City samples: ', city_samples)\n",
    "country_samples = country_enc.transform([sample[1] for sample in samples])\n",
    "print('Country samples: ', country_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (2, 2), (1, 2), (1, 0), (1, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "transformed_samples = list(zip(city_samples, country_samples))\n",
    "print(transformed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3]\n",
      "[0 4 7]\n",
      "['Atlanta', 'USA'] => [[1. 0. 0. 0. 0. 0. 1.]]\n",
      "['Charlotte', 'USA'] => [[0. 0. 1. 0. 0. 0. 1.]]\n",
      "['Baltimore', 'USA'] => [[0. 1. 0. 0. 0. 0. 1.]]\n",
      "['Baltimore', 'Germany'] => [[0. 1. 0. 0. 1. 0. 0.]]\n",
      "['Baltimore', 'Switzerland'] => [[0. 1. 0. 0. 0. 1. 0.]]\n",
      "['Zurich', 'Switzerland'] => [[0. 0. 0. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "enc.fit(transformed_samples)  \n",
    "print(enc.n_values_)\n",
    "print(enc.feature_indices_)\n",
    "for i in range(len(samples)):\n",
    "  feature_vector = enc.transform([transformed_samples[i]]).todense()\n",
    "  print('%s => %s' % (samples[i], feature_vector))\n",
    "#   print('city: %s, country: %s' % (feature_vector[enc.feature_indices_[0]:enc.feature_indices_[1]], \n",
    "#                                    feature_vector[enc.feature_indices_[1]:enc.feature_indices_[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Text\n",
    "\n",
    "Text data are particularly tricky to represent as vectors because they're so far removed from numerical data.\n",
    "\n",
    "The traditional approach here is what we call a \"bag of words\" model:\n",
    "\n",
    "* We take all the words in the corpus and assign them a number\n",
    "* We make a new vector where each index means a specific word (e.g. `v[0]` is `1` if the word `dog` is present and `0` otherwise)\n",
    "* We then take all of the documents and \"map\" each one to a vector by marking a `1` for every word position that's present\n",
    "\n",
    "This gives _surprisingly_ good results.\n",
    "\n",
    "There are a few variations here too:  \n",
    "\n",
    "* Use the frequency of words (`CountVectorizer`)\n",
    "* Use something called _tf-idf_ (`TfIdfVectorizer`) which accounts for the fact that some words are common in all documents, not just this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is lightly modified from the sklearn documentation.\n",
    "\n",
    "It illustrates the use of several new features of sklearn: `CountVectorizer`, `TfIdfTransformer`, and `Pipeline` which is a way to bundle up multiple steps, taken in order, for easier use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the boilerplate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to load the dataset and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "2018-04-18 16:28:36,579 INFO Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "2018-04-18 16:28:36,582 INFO Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n",
      "857 documents\n",
      "2 categories\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: Re: There must be a creator! (Maybe)\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: royc@rbdc.wsnc.org (Roy Crabtree)\\nSubje...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: Re: \"Imaginary\" Friends - Info and Ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...       0\n",
       "1  Subject: Re: There must be a creator! (Maybe)\\...       0\n",
       "2  From: MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka...       0\n",
       "3  From: royc@rbdc.wsnc.org (Roy Crabtree)\\nSubje...       1\n",
       "4  Subject: Re: \"Imaginary\" Friends - Info and Ex...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "#  Make a dataframe from this so it's easier to see.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data.data)\n",
    "df['target'] = pd.Series(data.target)\n",
    "df.columns = ['text', 'target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1),)}\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.522s\n",
      "\n",
      "Best score: 0.904\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#  Hide some deprecation warnings that are coming from inside sklearn.  \n",
    "#\n",
    "#  We can't actually fix these ourselves, so for now they're just noise\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1),), # (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(data.data, data.target)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example of a transformed text object:\n",
      "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x18053 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 53 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Here\\'s an example of a transformed text object:')\n",
    "steps = grid_search.best_estimator_.steps[0:-1]\n",
    "print(steps)\n",
    "vectorizer = Pipeline(steps)\n",
    "vectorizer.transform([data.data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's a row of over 18,000 values!  That's one index for each possible word in the vocabulary across all the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
